{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "daZm9aPyRUAM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### setup"
      ],
      "metadata": {
        "id": "CsZYuP9onEl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein\n",
        "!pip install einops\n",
        "!pip install einops_exts\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install tqdm\n",
        "!pip install sentencepiece\n",
        "# !pip install fair-esm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CpeO8QHj-_z5",
        "outputId": "2b964bf6-ad72-4d7b-8f3b-567e3ee22e06"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Levenshtein in /usr/local/lib/python3.11/dist-packages (0.27.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from Levenshtein) (3.12.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: einops_exts in /usr/local/lib/python3.11/dist-packages (0.0.4)\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.11/dist-packages (from einops_exts) (0.8.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from einops import rearrange, repeat\n",
        "# import esm\n",
        "\n",
        "# Set up GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# # Load ESM-2 model\n",
        "# esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "# batch_converter = alphabet.get_batch_converter()\n",
        "# esm_model = esm_model.to(device)  # Move to GPU if available\n",
        "# esm_model.eval()  # Set to evaluation mode\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CYInyPD_--Vo",
        "outputId": "b7f287b1-e673-4f5e-a2bf-38f7fa8a7fe6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data"
      ],
      "metadata": {
        "id": "B--CLs46QMLR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DyL8K36gQgPj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UGP9rWlQgn_",
        "outputId": "848c23a2-2fa4-4713-e7a0-d9384059ab0c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aN6Of5EMJ2Ju"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_snp_data(file_path):\n",
        "    snp_df = pd.read_csv(file_path)\n",
        "\n",
        "    # Basic preprocessing and length calculations\n",
        "    snp_df['smiles_length'] = snp_df['smiles'].apply(len)\n",
        "    snp_df['protein_length'] = snp_df['protein_sequence'].apply(len)\n",
        "\n",
        "    return snp_df\n",
        "\n",
        "def filter_datasets(dataset):\n",
        "    return dataset[\n",
        "        (dataset['smiles'].notna()) &\n",
        "        (dataset['protein_sequence'].notna()) &\n",
        "        (dataset['smiles_length'] > 0) &\n",
        "        (dataset['protein_length'] > 0)\n",
        "    ]\n",
        "\n",
        "class ProteinGenerationDataset(Dataset):\n",
        "    def __init__(self, dataframe, max_length):\n",
        "        self.dataframe = dataframe\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        return row['smiles'], row['protein_sequence']\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to handle padding within batches.\n",
        "    Args:\n",
        "        batch: List of tuples (smiles, protein)\n",
        "    Returns:\n",
        "        Padded and batched tensors\n",
        "    \"\"\"\n",
        "    smiles, proteins = zip(*batch)\n",
        "\n",
        "    # SMILES strings don't need padding as PolyBERT handles that internally\n",
        "    smiles = list(smiles)\n",
        "\n",
        "    # Get max length in this batch for proteins (not exceeding dataset max_length)\n",
        "    max_protein_len = min(max(len(p) for p in proteins), max_length)\n",
        "\n",
        "    # Pad proteins to max length in batch\n",
        "    padded_proteins = []\n",
        "    protein_masks = []\n",
        "\n",
        "    for protein in proteins:\n",
        "        if len(protein) > max_protein_len:\n",
        "            padded = protein[:max_protein_len]\n",
        "            mask = [1] * max_protein_len\n",
        "        else:\n",
        "            padded = protein + ' ' * (max_protein_len - len(protein))\n",
        "            mask = [1] * len(protein) + [0] * (max_protein_len - len(protein))\n",
        "\n",
        "        padded_proteins.append(padded)\n",
        "        protein_masks.append(mask)\n",
        "\n",
        "    return {\n",
        "        'smiles': smiles,\n",
        "        'proteins': padded_proteins,\n",
        "        'protein_masks': torch.tensor(protein_masks, dtype=torch.bool)\n",
        "    }"
      ],
      "metadata": {
        "id": "t6vR0DNfQRUZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### utils"
      ],
      "metadata": {
        "id": "Y84XLPeXQIEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Components\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0)]\n",
        "\n",
        "class DoublePositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        # Use the full embedding dimension divided into two halves\n",
        "        self.d_model = d_model\n",
        "        half_dim = d_model // 2\n",
        "\n",
        "        # Create position encodings for both input and output positions\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, half_dim, 2) * (-math.log(10000.0) / half_dim))\n",
        "\n",
        "        # Input position encodings\n",
        "        pe_input = torch.zeros(max_len, half_dim)\n",
        "        pe_input[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe_input[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Output position encodings\n",
        "        pe_output = torch.zeros(max_len, half_dim)\n",
        "        pe_output[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe_output[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe_input', pe_input)\n",
        "        self.register_buffer('pe_output', pe_output)\n",
        "\n",
        "    def forward(self, x, input_positions, output_positions):\n",
        "        batch_size, seq_length, _ = x.shape\n",
        "\n",
        "        # Create a tensor of zeros with the same shape as the input\n",
        "        pos_encoding = torch.zeros_like(x)\n",
        "\n",
        "        # For each item in the batch\n",
        "        for b in range(batch_size):\n",
        "            for t in range(seq_length):\n",
        "                # Get the input and output positions for this token\n",
        "                input_pos = input_positions[b, t] if input_positions is not None else t\n",
        "                output_pos = output_positions[b, t] if output_positions is not None else t\n",
        "\n",
        "                if input_pos < self.pe_input.size(0) and output_pos < self.pe_output.size(0):\n",
        "                    # Fill the first half with input position encoding\n",
        "                    pos_encoding[b, t, :self.d_model//2] = self.pe_input[input_pos]\n",
        "                    # Fill the second half with output position encoding\n",
        "                    pos_encoding[b, t, self.d_model//2:] = self.pe_output[output_pos]\n",
        "\n",
        "        return x + pos_encoding\n",
        "\n",
        "class PerceiverAttention(nn.Module):\n",
        "    def __init__(self, dim, dim_head=64, heads=8):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.norm_media = nn.LayerNorm(dim)\n",
        "        self.norm_latents = nn.LayerNorm(dim)\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
        "\n",
        "    def forward(self, x, latents):\n",
        "        \"\"\"\n",
        "        x: [batch_size, seq_len_x, dim]\n",
        "        latents: [batch_size, seq_len_l, dim]\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        x = self.norm_media(x)\n",
        "        latents = self.norm_latents(latents)\n",
        "\n",
        "        # Ensure latents has correct batch size\n",
        "        if latents.size(0) != batch_size:\n",
        "            latents = latents.expand(batch_size, -1, -1)\n",
        "\n",
        "        q = self.to_q(latents)\n",
        "        q = rearrange(q, 'b n (h d) -> b h n d', h=self.heads)\n",
        "        q = q * self.scale\n",
        "\n",
        "        # Ensure proper concatenation\n",
        "        kv_input = torch.cat((x, latents), dim=1)  # concatenate along sequence dimension\n",
        "        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n",
        "        k = rearrange(k, 'b n (h d) -> b h n d', h=self.heads)\n",
        "        v = rearrange(v, 'b n (h d) -> b h n d', h=self.heads)\n",
        "\n",
        "        sim = torch.einsum('b h i d, b h j d -> b h i j', q, k)\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = torch.einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "\n",
        "        return self.to_out(out)\n",
        "\n",
        "class GatedCrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_head=64, heads=8, ff_mult=4):\n",
        "        super().__init__()\n",
        "        self.attn = PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads)\n",
        "        self.attn_gate = nn.Parameter(torch.tensor([0.]))\n",
        "        self.ff = FeedForward(dim, mult=ff_mult)\n",
        "        self.ff_gate = nn.Parameter(torch.tensor([0.]))\n",
        "\n",
        "    def forward(self, x, media):\n",
        "        \"\"\"\n",
        "        x: [batch_size, seq_len_x, dim]\n",
        "        media: [batch_size, seq_len_m, dim]\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        target_batch_size = media.size(0)\n",
        "\n",
        "        # Handle batch size mismatch\n",
        "        if batch_size > target_batch_size:\n",
        "            media = media.expand(batch_size, -1, -1)\n",
        "        elif batch_size < target_batch_size:\n",
        "            x = x.expand(target_batch_size, -1, -1)\n",
        "\n",
        "        gate = self.attn_gate.tanh()\n",
        "        x = self.attn(media, x) * gate + x\n",
        "        x = self.ff(x) * self.ff_gate.tanh() + x\n",
        "        return x\n",
        "\n",
        "class PerceiverResampler(nn.Module):\n",
        "    def __init__(self, dim, depth, dim_head=64, heads=8, num_latents=64):\n",
        "        super().__init__()\n",
        "        # Initialize latents without batch dimension\n",
        "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n",
        "                FeedForward(dim=dim)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        # Expand latents to batch size\n",
        "        latents = repeat(self.latents, 'n d -> b n d', b=batch_size)\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            latents = attn(x, latents) + latents\n",
        "            latents = ff(latents) + latents\n",
        "\n",
        "        return latents\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, dim * mult, bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim * mult, dim, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# class PerceiverResampler(nn.Module):\n",
        "#     def __init__(self, dim, depth, dim_head=64, heads=8, num_latents=64):\n",
        "#         super().__init__()\n",
        "#         self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
        "#         self.layers = nn.ModuleList([])\n",
        "\n",
        "#         for _ in range(depth):\n",
        "#             self.layers.append(nn.ModuleList([\n",
        "#                 PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n",
        "#                 FeedForward(dim=dim)\n",
        "#             ]))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         latents = repeat(self.latents, 'n d -> b n d', b=x.shape[0])\n",
        "\n",
        "#         for attn, ff in self.layers:\n",
        "#             latents = attn(x, latents) + latents\n",
        "#             latents = ff(latents) + latents\n",
        "\n",
        "#         return latents\n",
        "\n",
        "# class GatedCrossAttentionBlock(nn.Module):\n",
        "#     def __init__(self, dim, dim_head=64, heads=8, ff_mult=4):\n",
        "#         super().__init__()\n",
        "#         self.attn = PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads)\n",
        "#         self.attn_gate = nn.Parameter(torch.tensor([0.]))\n",
        "#         self.ff = FeedForward(dim, mult=ff_mult)\n",
        "#         self.ff_gate = nn.Parameter(torch.tensor([0.]))\n",
        "\n",
        "#     def forward(self, x, media):\n",
        "#         gate = self.attn_gate.tanh()\n",
        "#         x = self.attn(media, x) * gate + x\n",
        "#         x = self.ff(x) * self.ff_gate.tanh() + x\n",
        "#         return x"
      ],
      "metadata": {
        "id": "XEODw7FYQJEy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PolyBert Encoder"
      ],
      "metadata": {
        "id": "daZm9aPyRUAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "00Y8KtCzni07"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class PolyBERTEncoder(nn.Module):\n",
        "#     def __init__(self, output_dim):\n",
        "#         super().__init__()\n",
        "#         self.polybert = AutoModel.from_pretrained('kuelumbus/polyBERT')\n",
        "#         self.tokenizer = AutoTokenizer.from_pretrained('kuelumbus/polyBERT')\n",
        "#         self.output_dim = output_dim\n",
        "#         # Add a projection layer to match the required dimension\n",
        "#         self.projection = nn.Linear(self.polybert.config.hidden_size, output_dim)\n",
        "\n",
        "#     def mean_pooling(self, model_output, attention_mask):\n",
        "#         token_embeddings = model_output[0]\n",
        "#         input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "#         return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "#     def forward(self, smiles_strings):\n",
        "#         # Tokenize the SMILES strings\n",
        "#         encoded_input = self.tokenizer(smiles_strings,\n",
        "#                                      padding=True,\n",
        "#                                      truncation=True,\n",
        "#                                      return_tensors='pt').to(next(self.polybert.parameters()).device)\n",
        "\n",
        "#         # Get PolyBERT embeddings\n",
        "#         with torch.no_grad():\n",
        "#             model_output = self.polybert(**encoded_input)\n",
        "\n",
        "#         # Debug prints\n",
        "#         print(\"Model Output Keys:\", model_output.keys())  # Check available keys\n",
        "#         # print(\"Last Hidden State:\", model_output.last_hidden_state)\n",
        "#         print(\"Last Hidden State Shape:\", model_output.last_hidden_state.shape)\n",
        "\n",
        "#         # Pool the embeddings\n",
        "#         pooled_output = self.mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "\n",
        "#         # print(\"Pooled Output:\", pooled_output)\n",
        "#         print(\"Pooled Output Shape:\", pooled_output.shape)\n",
        "\n",
        "#         # Project to required dimension\n",
        "#         projected_output = self.projection(pooled_output)\n",
        "\n",
        "#         return projected_output"
      ],
      "metadata": {
        "id": "rj3wpaKHRVmv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolyBERTEncoder(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super().__init__()\n",
        "        self.polybert = AutoModel.from_pretrained('kuelumbus/polyBERT')\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('kuelumbus/polyBERT')\n",
        "        self.output_dim = output_dim\n",
        "        # Project each token embedding to required dimension\n",
        "        self.projection = nn.Linear(self.polybert.config.hidden_size, output_dim)\n",
        "\n",
        "    def forward(self, smiles_strings):\n",
        "        # Tokenize the SMILES strings\n",
        "        encoded_input = self.tokenizer(smiles_strings,\n",
        "                                     padding=True,\n",
        "                                     truncation=True,\n",
        "                                     return_tensors='pt').to(next(self.polybert.parameters()).device)\n",
        "\n",
        "        # Get PolyBERT embeddings\n",
        "        with torch.no_grad():\n",
        "            model_output = self.polybert(**encoded_input)\n",
        "\n",
        "        # Debug prints\n",
        "        # print(\"Model Output Keys:\", model_output.keys())\n",
        "        # print(\"Last Hidden State Shape:\", model_output.last_hidden_state.shape)  # [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        # Get sequence embeddings\n",
        "        sequence_embeddings = model_output.last_hidden_state\n",
        "\n",
        "        # Project each token embedding to required dimension\n",
        "        projected_output = self.projection(sequence_embeddings)  # [batch_size, seq_len, output_dim]\n",
        "        # print(\"Projected Output Shape:\", projected_output.shape)\n",
        "\n",
        "        return projected_output"
      ],
      "metadata": {
        "id": "-2VWy9ecrDNX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ProtFlamingo"
      ],
      "metadata": {
        "id": "0Pe8VJFxQSS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "d3FCpZZidXy3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "c1CcCRv-9vxW"
      },
      "outputs": [],
      "source": [
        "class SigmaProtFlamingo(nn.Module):\n",
        "    def __init__(self, model_path, max_len, cross_attn_every=3, dim_head=64, heads=8, perceiver_depth=2, perceiver_num_latents=64):\n",
        "        super().__init__()\n",
        "\n",
        "        self.protGPT2_model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "        self.protGPT2_tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "        self.max_len = max_len\n",
        "\n",
        "        if self.protGPT2_tokenizer.pad_token is None:\n",
        "            self.protGPT2_tokenizer.pad_token = self.protGPT2_tokenizer.eos_token\n",
        "            self.protGPT2_model.config.pad_token_id = self.protGPT2_model.config.eos_token_id\n",
        "\n",
        "        self.cross_attn_every = cross_attn_every\n",
        "\n",
        "        # PolyBERT encoder for SMILES strings\n",
        "        self.polybert_encoder = PolyBERTEncoder(self.protGPT2_model.config.n_embd)\n",
        "\n",
        "        # Replace single positional encoding with double positional encoding\n",
        "        self.positional_encoding = DoublePositionalEncoding(self.protGPT2_model.config.n_embd, max_len=max_len)\n",
        "\n",
        "        # Single perceiver resampler for SMILES embeddings\n",
        "        self.smiles_perceiver = PerceiverResampler(\n",
        "            dim=self.protGPT2_model.config.n_embd,\n",
        "            depth=perceiver_depth,\n",
        "            dim_head=dim_head,\n",
        "            heads=heads,\n",
        "            num_latents=perceiver_num_latents\n",
        "        )\n",
        "\n",
        "        # Cross attention layers\n",
        "        num_gpt_layers = len(self.protGPT2_model.transformer.h)\n",
        "        self.cross_attn = nn.ModuleList([\n",
        "            GatedCrossAttentionBlock(dim=self.protGPT2_model.config.n_embd, dim_head=dim_head, heads=heads)\n",
        "            for _ in range(num_gpt_layers)\n",
        "        ])\n",
        "\n",
        "        # Combine GPT layers with cross attention\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i, block in enumerate(self.protGPT2_model.transformer.h):\n",
        "            self.layers.append(block)\n",
        "            if i % cross_attn_every == 0 and i != 0:\n",
        "                self.layers.append(GatedCrossAttentionBlock(dim=self.protGPT2_model.config.n_embd, dim_head=dim_head, heads=heads))\n",
        "\n",
        "    def forward(self, smiles_strings, order=None, targets=None, optimize=False, kv_cache=None, burst=False):\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        # Get SMILES embeddings through PolyBERT\n",
        "        smiles_embeddings = self.polybert_encoder(smiles_strings)\n",
        "        processed_smiles = self.smiles_perceiver(smiles_embeddings)\n",
        "\n",
        "        # Initialize with start token\n",
        "        gpt_input = self.protGPT2_tokenizer.encode_plus(\n",
        "            \"<|endoftext|>\",\n",
        "            return_tensors=\"pt\",\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            truncation=True\n",
        "        ).to(device)\n",
        "\n",
        "        input_ids = gpt_input.input_ids.long()\n",
        "        seq_length = input_ids.size(1)\n",
        "        batch_size = 1 if isinstance(smiles_strings, str) else len(smiles_strings)\n",
        "\n",
        "        hidden_states = self.protGPT2_model.transformer.wte(input_ids)\n",
        "\n",
        "        # If no order is provided, use left-to-right\n",
        "        if order is None:\n",
        "            order = torch.arange(seq_length, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "\n",
        "        # Make sure order is the right length\n",
        "        if order.size(1) > seq_length:\n",
        "            order = order[:, :seq_length]\n",
        "        elif order.size(1) < seq_length:\n",
        "            # Pad order if needed\n",
        "            padding = torch.arange(order.size(1), seq_length, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "            order = torch.cat([order, padding], dim=1)\n",
        "\n",
        "        # Map the input tokens according to the order\n",
        "        # When using random order, we need to reshuffle the input tokens\n",
        "        if not optimize and not burst:  # Only shuffle during training\n",
        "            reordered_input_ids = torch.zeros_like(input_ids)\n",
        "            for b in range(batch_size):\n",
        "                # Reorder the input tokens according to the order\n",
        "                reordered_input_ids[b] = input_ids[b, order[b]]\n",
        "\n",
        "            # Re-embed with reordered tokens\n",
        "            hidden_states = self.protGPT2_model.transformer.wte(reordered_input_ids)\n",
        "\n",
        "        # Get input and output positions from the order\n",
        "        # Input positions: the current position in the order\n",
        "        # Output positions: the next position in the order\n",
        "        input_positions = order\n",
        "        # Shift the order by 1 to get output positions (target positions)\n",
        "        output_positions = torch.roll(order, -1, dims=1)\n",
        "        # The last position wraps to the first position\n",
        "        output_positions[:, -1] = order[:, 0]\n",
        "\n",
        "        # Apply double positional encoding\n",
        "        hidden_states = self.positional_encoding(hidden_states, input_positions, output_positions)\n",
        "\n",
        "        # Create attention mask based on the order\n",
        "        attention_mask = gpt_input.attention_mask\n",
        "        num_heads = self.protGPT2_model.config.n_head\n",
        "\n",
        "        # Create 4D attention mask [batch_size, num_heads, seq_length, seq_length]\n",
        "        attention_mask = attention_mask.view(batch_size, 1, 1, seq_length)\n",
        "        attention_mask = attention_mask.expand(batch_size, num_heads, seq_length, seq_length)\n",
        "        attention_mask = attention_mask.to(dtype=hidden_states.dtype)\n",
        "\n",
        "        # Create causal mask based on the order\n",
        "        # A token at position i can attend to tokens at positions j where order[j] <= order[i]\n",
        "        causal_mask = torch.zeros((batch_size, seq_length, seq_length), device=device)\n",
        "        for b in range(batch_size):\n",
        "            for i in range(seq_length):\n",
        "                for j in range(seq_length):\n",
        "                    # Token i can attend to token j if order[j] <= order[i]\n",
        "                    # This means j comes before or at the same position as i in the order\n",
        "                    if order[b, j] <= order[b, i]:\n",
        "                        causal_mask[b, i, j] = 1.0\n",
        "\n",
        "        # Reshape causal_mask to match attention_mask and combine them\n",
        "        causal_mask = causal_mask.unsqueeze(1)  # [batch_size, 1, seq_length, seq_length]\n",
        "        combined_mask = attention_mask * causal_mask\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if isinstance(layer, GatedCrossAttentionBlock):\n",
        "                hidden_states = layer(hidden_states, processed_smiles)\n",
        "            else:\n",
        "                hidden_states = layer(hidden_states, attention_mask=combined_mask)[0]\n",
        "\n",
        "        # Get logits\n",
        "        logits = self.protGPT2_model.lm_head(hidden_states)\n",
        "\n",
        "        if targets is None:\n",
        "            if optimize:\n",
        "                # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "                return logits[:, [-1], :], None\n",
        "            return logits, None\n",
        "\n",
        "        # Compute loss against the targets\n",
        "        # If targets are provided in original order, we need to shuffle them to match our order\n",
        "        if targets is not None:\n",
        "            shuffled_targets = torch.zeros_like(targets)\n",
        "            for b in range(batch_size):\n",
        "                # Reorder the targets according to the order\n",
        "                shuffled_targets[b] = targets[b, order[b]]\n",
        "\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                shuffled_targets.view(-1),\n",
        "                ignore_index=-1\n",
        "            )\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def custom_generate(self, smiles_string, max_length=200):\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        # Get SMILES embeddings\n",
        "        smiles_embeddings = self.polybert_encoder(smiles_string)\n",
        "        processed_smiles = self.smiles_perceiver(smiles_embeddings)\n",
        "\n",
        "        # Initialize with start token\n",
        "        input_ids = torch.tensor([[self.protGPT2_tokenizer.bos_token_id]]).to(device)\n",
        "\n",
        "        # Autoregressive generation\n",
        "        for _ in range(max_length):\n",
        "            inputs_embeds = self.protGPT2_model.transformer.wte(input_ids)\n",
        "            inputs_embeds = self.positional_encoding(inputs_embeds)\n",
        "\n",
        "            hidden_states = inputs_embeds\n",
        "            cross_attn_idx = 0\n",
        "\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                if isinstance(layer, GatedCrossAttentionBlock):\n",
        "                    hidden_states = layer(hidden_states, processed_smiles)\n",
        "                    cross_attn_idx += 1\n",
        "                else:\n",
        "                    hidden_states = layer(hidden_states, attention_mask=None)[0]\n",
        "\n",
        "            next_token_logits = self.protGPT2_model.lm_head(hidden_states[:, -1, :])\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
        "\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "\n",
        "            if next_token.item() == self.protGPT2_tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        return self.protGPT2_tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    def generate(self, smiles_string, max_length=50):\n",
        "        return self.custom_generate(smiles_string, max_length)\n",
        "\n",
        "    def state_dict(self):\n",
        "        state_dict = super().state_dict()\n",
        "        state_dict['smiles_perceiver'] = self.smiles_perceiver.state_dict()\n",
        "        state_dict['cross_attn'] = self.cross_attn.state_dict()\n",
        "        state_dict['polybert_encoder'] = self.polybert_encoder.state_dict()\n",
        "        return state_dict\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        smiles_perceiver_state = state_dict.pop('smiles_perceiver')\n",
        "        cross_attn_state = state_dict.pop('cross_attn')\n",
        "        polybert_encoder_state = state_dict.pop('polybert_encoder')\n",
        "\n",
        "        super().load_state_dict(state_dict)\n",
        "\n",
        "        self.smiles_perceiver.load_state_dict(smiles_perceiver_state)\n",
        "        self.cross_attn.load_state_dict(cross_attn_state)\n",
        "        self.polybert_encoder.load_state_dict(polybert_encoder_state)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "_wnL43mTZkIN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### training"
      ],
      "metadata": {
        "id": "6o91I0EXQaF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "wWmS0warQa-n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_random_order(model, train_loader, val_loader, num_epochs, device,\n",
        "                           curriculum_steps=0, l2_reg=1e-5, sample_smiles=None):\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=l2_reg)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=model.protGPT2_tokenizer.pad_token_id)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    loss_log = []\n",
        "    checkpoint_path = \"/content/drive/MyDrive/classes+projects/plastic_enzyme_project/2024/codes/sigma_checkpoint.pth\"\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(\"Loading checkpoint...\")\n",
        "        model.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "    # Total training steps for curriculum\n",
        "    total_steps = num_epochs * len(train_loader)\n",
        "    step_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        batch_losses = []\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            step_counter += 1\n",
        "\n",
        "            smiles_strings = batch['smiles']\n",
        "            proteins = batch['proteins']\n",
        "            protein_masks = batch['protein_masks'].to(device)\n",
        "\n",
        "            # Determine whether to use random or left-to-right order based on curriculum\n",
        "            if curriculum_steps > 0:\n",
        "                # Current percentage of random ordering\n",
        "                random_prob = min(1.0, step_counter / curriculum_steps)\n",
        "                use_random = random.random() < random_prob\n",
        "            else:\n",
        "                use_random = True\n",
        "\n",
        "            batch_size = len(smiles_strings)\n",
        "            seq_length = model.max_len\n",
        "\n",
        "            # Generate order\n",
        "            if use_random:\n",
        "                # Create random permutation for each batch item\n",
        "                order = torch.stack([torch.randperm(seq_length) for _ in range(batch_size)]).to(device)\n",
        "            else:\n",
        "                # Left-to-right order\n",
        "                order = torch.arange(seq_length).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Encode the proteins\n",
        "            target_encoding = model.protGPT2_tokenizer(\n",
        "                proteins,\n",
        "                return_tensors=\"pt\",\n",
        "                padding='max_length',\n",
        "                max_length=model.max_len,\n",
        "                truncation=True\n",
        "            ).to(device)\n",
        "\n",
        "            # Forward pass with order - the targets are already in original order\n",
        "            # The model will handle shuffling the targets according to the order\n",
        "            outputs, loss = model(\n",
        "                smiles_strings,\n",
        "                order=order,\n",
        "                targets=target_encoding.input_ids\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_losses.append(loss.item())\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "        print(f\"Per-batch Losses: {batch_losses[:5]} ...\")\n",
        "\n",
        "        val_loss = validate_with_random_order(model, val_loader, criterion, device)\n",
        "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "        loss_log.append({\n",
        "            'epoch': epoch+1,\n",
        "            'train_loss': avg_loss,\n",
        "            'val_loss': val_loss\n",
        "        })\n",
        "\n",
        "        scheduler.step()\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "        # Generate sample proteins after each epoch\n",
        "        if sample_smiles:\n",
        "            print(\"\\nSample Generated Proteins:\")\n",
        "            for smiles in sample_smiles:\n",
        "                # Try both random and left-to-right generation\n",
        "                gen_protein_lr = generate_autoregressively(model, smiles, max_length=100, random_order=False)\n",
        "                gen_protein_random = generate_autoregressively(model, smiles, max_length=100, random_order=True)\n",
        "                print(f\"SMILES: {smiles}\")\n",
        "                print(f\"Generated Protein (L2R): {gen_protein_lr}\")\n",
        "                print(f\"Generated Protein (Random): {gen_protein_random}\\n\")\n",
        "\n",
        "    # Save loss log\n",
        "    loss_df = pd.DataFrame(loss_log)\n",
        "    loss_df.to_csv(\"/content/drive/MyDrive/classes+projects/plastic_enzyme_project/2024/codes/sigma_loss_log.csv\", index=False)\n",
        "\n",
        "    # Plot loss\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(loss_df['epoch'], loss_df['train_loss'], label='Train Loss', marker='o')\n",
        "    plt.plot(loss_df['epoch'], loss_df['val_loss'], label='Validation Loss', marker='s')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.savefig(\"sigma_loss_plot.png\")\n",
        "    plt.show()\n",
        "\n",
        "def validate_with_random_order(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            smiles_strings = batch['smiles']\n",
        "            proteins = batch['proteins']\n",
        "            protein_masks = batch['protein_masks'].to(device)\n",
        "\n",
        "            batch_size = len(smiles_strings)\n",
        "            seq_length = model.max_len\n",
        "\n",
        "            # Create random order for each item in the batch\n",
        "            order = torch.stack([torch.randperm(seq_length) for _ in range(batch_size)]).to(device)\n",
        "\n",
        "            # Encode the proteins\n",
        "            target_encoding = model.protGPT2_tokenizer(\n",
        "                proteins,\n",
        "                return_tensors=\"pt\",\n",
        "                padding='max_length',\n",
        "                max_length=model.max_len,\n",
        "                truncation=True\n",
        "            ).to(device)\n",
        "\n",
        "            # Forward pass with order\n",
        "            outputs, loss = model(\n",
        "                smiles_strings,\n",
        "                order=order,\n",
        "                targets=target_encoding.input_ids\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(val_loader)"
      ],
      "metadata": {
        "id": "M6WRIYHEfN6y"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### inference + training"
      ],
      "metadata": {
        "id": "CfYBZUMhfOTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = preprocess_snp_data('/content/augmented_train.csv')\n",
        "val_data = preprocess_snp_data('/content/augmented_val.csv')\n",
        "test_data = preprocess_snp_data('/content/augmented_test.csv')\n",
        "\n",
        "train_data = filter_datasets(train_data)\n",
        "val_data = filter_datasets(val_data)\n",
        "test_data = filter_datasets(test_data)\n",
        "\n",
        "# Calculate max sequence length\n",
        "max_length = max(\n",
        "    train_data['protein_length'].max(),\n",
        "    val_data['protein_length'].max(),\n",
        "    test_data['protein_length'].max()\n",
        ")\n",
        "max_length = min(max_length, 1024)  # Cap at 1024 or your desired maximum\n",
        "print(f\"Max sequence length: {max_length}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ProteinGenerationDataset(train_data, max_length)\n",
        "val_dataset = ProteinGenerationDataset(val_data, max_length)\n",
        "test_dataset = ProteinGenerationDataset(test_data, max_length)\n",
        "\n",
        "# Create dataloaders with custom collate function\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,  # Adjust based on your GPU memory\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Initialize model with sigma-gpt capabilities\n",
        "model = SigmaProtFlamingo(\n",
        "    model_path='nferruz/ProtGPT2',\n",
        "    max_len=max_length,\n",
        "    cross_attn_every=3,\n",
        "    dim_head=64,\n",
        "    heads=8,\n",
        "    perceiver_depth=2,\n",
        "    perceiver_num_latents=64\n",
        ").to(device)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop with curriculum learning\n",
        "# Start with 50% of sequences in left-to-right order and gradually increase to 100% random\n",
        "curriculum_steps = int(0.5 * num_epochs * len(train_loader))  # Curriculum over first half of training\n",
        "print(\"Starting training with sigma-gpt capabilities...\")\n",
        "train_with_random_order(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs,\n",
        "    device,\n",
        "    curriculum_steps=curriculum_steps\n",
        ")\n",
        "\n",
        "# # Generate and evaluate\n",
        "# print(\"Generating proteins for test set...\")\n",
        "# test_results = generate_and_evaluate(model, test_loader, device)\n",
        "\n",
        "# # Save results\n",
        "# print(\"Saving results...\")\n",
        "# results_path = '/content/drive/MyDrive/classes+projects/plastic_enzyme_project/2024/codes/test_results.json'\n",
        "# with open(results_path, 'w') as f:\n",
        "#     json.dump(test_results, f, indent=2)\n",
        "\n",
        "# print(f\"Results saved to {results_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-67RvdafPdi",
        "outputId": "ae500967-e408-424c-b4d6-097f726e3c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Max sequence length: 914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training with sigma-gpt capabilities...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/10:   0%|          | 0/3111 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hsqe5zYSJ5Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### generation"
      ],
      "metadata": {
        "id": "sLZXFhNq9xvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the trained model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Path to model checkpoint\n",
        "checkpoint_path = \"/content/drive/MyDrive/classes+projects/plastic_enzyme_project/2024/codes/latest_checkpoint.pth\"\n",
        "\n",
        "# Load model\n",
        "model = ProtFlamingo(\n",
        "    model_path='nferruz/ProtGPT2',\n",
        "    max_len=914,  # Ensure this matches the training max_len\n",
        "    cross_attn_every=3,\n",
        "    dim_head=64,\n",
        "    heads=8,\n",
        "    perceiver_depth=2,\n",
        "    perceiver_num_latents=64\n",
        ").to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "vXlyfekIsTlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ProteinGenerationDataset"
      ],
      "metadata": {
        "id": "OS0GAkWg-aV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained weights\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Load test data\n",
        "test_data = preprocess_snp_data('/content/augmented_test.csv')\n",
        "test_data = filter_datasets(test_data)\n",
        "\n",
        "# Create test dataset and dataloader\n",
        "test_dataset = ProteinGenerationDataset(test_data,max_length = 914 )\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "JHBmfbMT-FPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Function to compare generated vs. ground truth proteins for unique SMILES\n",
        "# def evaluate_generation(model, test_loader, device, output_file=\"generated_vs_ground_truth.json\"):\n",
        "#     model.eval()\n",
        "#     results = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         # Collect unique SMILES from test_loader\n",
        "#         unique_smiles = set()\n",
        "#         smiles_to_target = {}  # Map unique SMILES to ground truth\n",
        "\n",
        "#         for batch in test_loader:\n",
        "#             for smiles, target_protein in zip(batch['smiles'], batch['proteins']):\n",
        "#                 unique_smiles.add(smiles)\n",
        "#                 smiles_to_target[smiles] = target_protein.strip()  # Store ground truth\n",
        "\n",
        "#         unique_smiles = list(unique_smiles)  # Convert set to list\n",
        "#         print(f\"Total Unique SMILES: {len(unique_smiles)}\")\n",
        "\n",
        "#         # Generate proteins for unique SMILES\n",
        "#         for smiles in tqdm(unique_smiles, desc=\"Generating Proteins for Unique SMILES\"):\n",
        "#             generated_proteins = [model.generate(smiles, max_length=914) for _ in range(1)]  # Generate 1 sequences\n",
        "\n",
        "#             print(f\"\\nSMILES: {smiles}\")\n",
        "#             for i, gen_protein in enumerate(generated_proteins):\n",
        "#                 print(f\"Generated Protein {i+1}: {gen_protein}\")\n",
        "#             print(f\"Ground Truth Protein: {smiles_to_target[smiles]}\")\n",
        "\n",
        "#             results.append({\n",
        "#                 'SMILES': smiles,\n",
        "#                 'Generated Proteins': generated_proteins,\n",
        "#                 'Ground Truth Protein': smiles_to_target[smiles]\n",
        "#             })\n",
        "\n",
        "#     # Save results to a JSON file\n",
        "#     with open(output_file, 'w') as f:\n",
        "#         json.dump(results, f, indent=2)\n",
        "\n",
        "#     print(f\"\\nResults saved to {output_file}\")\n",
        "#     return results\n",
        "\n",
        "# # Run evaluation\n",
        "# output_path = \"/content/drive/MyDrive/classes+projects/plastic_enzyme_project/2024/codes/generated_vs_ground_truth.json\"\n",
        "# test_results = evaluate_generation(model, test_loader, device, output_file=output_path)\n",
        "\n",
        "# # Print first few examples\n",
        "# print(\"\\nExample Comparisons:\")\n",
        "# for i, result in enumerate(test_results[:5]):  # Show first 5 examples\n",
        "#     print(f\"\\nExample {i+1}:\")\n",
        "#     print(f\"SMILES: {result['SMILES']}\")\n",
        "#     for j, gen_protein in enumerate(result['Generated Proteins']):\n",
        "#         print(f\"Generated Protein {j+1}: {gen_protein}\")\n",
        "#     print(f\"Ground Truth Protein: {result['Ground Truth Protein']}\")\n"
      ],
      "metadata": {
        "id": "N86ICdmH-HeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_autoregressively(model, smiles_string, max_length=100, temperature=1.0, random_order=False):\n",
        "    \"\"\"Generate protein autoregressively, with option to use random order\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Get SMILES embeddings\n",
        "    smiles_embeddings = model.polybert_encoder([smiles_string])\n",
        "    processed_smiles = model.smiles_perceiver(smiles_embeddings)\n",
        "\n",
        "    # Initialize with start token\n",
        "    input_ids = torch.tensor([[model.protGPT2_tokenizer.bos_token_id]], device=device)\n",
        "\n",
        "    # If using random order, generate a random permutation\n",
        "    if random_order:\n",
        "        order = torch.randperm(max_length, device=device).unsqueeze(0)\n",
        "    else:\n",
        "        order = torch.arange(max_length, device=device).unsqueeze(0)\n",
        "\n",
        "    # Track the current positions in the order\n",
        "    current_pos = 0\n",
        "\n",
        "    # Generated sequence in order's positions\n",
        "    generated_sequence = torch.full((1, max_length), model.protGPT2_tokenizer.pad_token_id, device=device)\n",
        "    generated_sequence[0, 0] = model.protGPT2_tokenizer.bos_token_id  # Start token\n",
        "\n",
        "    while current_pos < max_length - 1:\n",
        "        # Get the next position in the order\n",
        "        next_pos = current_pos + 1\n",
        "\n",
        "        # Forward pass to get next token prediction\n",
        "        with torch.no_grad():\n",
        "            # Use only the sequence up to the current position\n",
        "            current_order = order[:, :next_pos]\n",
        "            current_sequence = generated_sequence[:, current_order[0]]\n",
        "\n",
        "            # Get logits for the next token\n",
        "            logits, _ = model(\n",
        "                smiles_string,\n",
        "                order=current_order,\n",
        "                optimize=True\n",
        "            )\n",
        "\n",
        "            # Apply temperature and sample\n",
        "            logits = logits[0, -1, :] / temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            # Add the token to the generated sequence at the next position in the order\n",
        "            generated_sequence[0, order[0, next_pos]] = next_token\n",
        "\n",
        "            # Check for EOS token\n",
        "            if next_token == model.protGPT2_tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            current_pos = next_pos\n",
        "\n",
        "    # Decode the generated sequence\n",
        "    generated_ids = generated_sequence[0].tolist()\n",
        "    # Remove padding tokens\n",
        "    generated_ids = [id for id in generated_ids if id != model.protGPT2_tokenizer.pad_token_id]\n",
        "    return model.protGPT2_tokenizer.decode(generated_ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "Sgmyq7fJ8kAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_rejection_sampling(model, smiles_string, max_length=100, num_orders=5, temperature=1.0):\n",
        "    \"\"\"Generate protein using token-based rejection sampling (burst mode)\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Get SMILES embeddings\n",
        "    smiles_embeddings = model.polybert_encoder([smiles_string])\n",
        "    processed_smiles = model.smiles_perceiver(smiles_embeddings)\n",
        "\n",
        "    # Initialize with start token\n",
        "    generated_sequence = torch.full((1, max_length), model.protGPT2_tokenizer.pad_token_id, device=device)\n",
        "    generated_sequence[0, 0] = model.protGPT2_tokenizer.bos_token_id  # Start token\n",
        "\n",
        "    # Track filled positions\n",
        "    filled_positions = {0}  # Start with position 0 filled\n",
        "\n",
        "    while len(filled_positions) < max_length:\n",
        "        remaining_positions = [i for i in range(max_length) if i not in filled_positions]\n",
        "        if not remaining_positions:\n",
        "            break\n",
        "\n",
        "        # Sample tokens for all remaining positions\n",
        "        candidate_tokens = {}\n",
        "        for pos in remaining_positions:\n",
        "            # Create a temporary sequence with this position as the target\n",
        "            temp_order = torch.tensor([[pos]], device=device)\n",
        "\n",
        "            # Get logits for this position\n",
        "            with torch.no_grad():\n",
        "                logits, _ = model(\n",
        "                    smiles_string,\n",
        "                    order=temp_order,\n",
        "                    optimize=True\n",
        "                )\n",
        "\n",
        "                # Sample a token\n",
        "                logits = logits[0, -1, :] / temperature\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                token = torch.multinomial(probs, num_samples=1).item()\n",
        "                candidate_tokens[pos] = token\n",
        "\n",
        "        # Now evaluate acceptance for these candidates under different orders\n",
        "        best_accepted = []\n",
        "\n",
        "        for _ in range(num_orders):\n",
        "            # Create a random order of the remaining positions\n",
        "            rand_order = random.sample(remaining_positions, len(remaining_positions))\n",
        "\n",
        "            # Try to accept tokens in this order\n",
        "            accepted_positions = []\n",
        "\n",
        "            for pos in rand_order:\n",
        "                # Create a sequence with all previously accepted tokens\n",
        "                temp_sequence = generated_sequence.clone()\n",
        "                for accepted_pos in accepted_positions:\n",
        "                    temp_sequence[0, accepted_pos] = candidate_tokens[accepted_pos]\n",
        "\n",
        "                # Evaluate probability under this context\n",
        "                filled_pos_list = sorted(list(filled_positions) + accepted_positions)\n",
        "                context_order = torch.tensor([filled_pos_list + [pos]], device=device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    logits, _ = model(\n",
        "                        smiles_string,\n",
        "                        order=context_order,\n",
        "                        optimize=True\n",
        "                    )\n",
        "\n",
        "                    # Get probability of the candidate token\n",
        "                    logits = logits[0, -1, :] / temperature\n",
        "                    probs = F.softmax(logits, dim=-1)\n",
        "                    prob = probs[candidate_tokens[pos]].item()\n",
        "\n",
        "                    # Decision to accept - simplified for now\n",
        "                    if random.random() < min(1.0, prob * len(probs)):\n",
        "                        accepted_positions.append(pos)\n",
        "                    else:\n",
        "                        # Stop at first rejection\n",
        "                        break\n",
        "\n",
        "            # Keep track of the best set of accepted positions\n",
        "            if len(accepted_positions) > len(best_accepted):\n",
        "                best_accepted = accepted_positions\n",
        "\n",
        "        # Update the generated sequence with the best accepted tokens\n",
        "        for pos in best_accepted:\n",
        "            generated_sequence[0, pos] = candidate_tokens[pos]\n",
        "            filled_positions.add(pos)\n",
        "\n",
        "            # Check for EOS token\n",
        "            if candidate_tokens[pos] == model.protGPT2_tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        # If nothing was accepted, just accept one token to avoid getting stuck\n",
        "        if not best_accepted:\n",
        "            pos = remaining_positions[0]\n",
        "            generated_sequence[0, pos] = candidate_tokens[pos]\n",
        "            filled_positions.add(pos)\n",
        "\n",
        "    # Reorder the sequence to standard order\n",
        "    ordered_sequence = generated_sequence[0].tolist()\n",
        "\n",
        "    # Decode and return\n",
        "    # Remove padding tokens\n",
        "    ordered_sequence = [id for id in ordered_sequence if id != model.protGPT2_tokenizer.pad_token_id]\n",
        "    return model.protGPT2_tokenizer.decode(ordered_sequence, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "Koy4bjRHu5hx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}