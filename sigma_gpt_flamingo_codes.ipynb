{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "CsZYuP9onEl1",
        "B--CLs46QMLR",
        "Y84XLPeXQIEv",
        "daZm9aPyRUAM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### setup"
      ],
      "metadata": {
        "id": "CsZYuP9onEl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein\n",
        "!pip install einops\n",
        "!pip install einops_exts\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install tqdm\n",
        "!pip install sentencepiece\n",
        "# !pip install fair-esm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CpeO8QHj-_z5",
        "outputId": "6583035d-01f8-4c57-b4aa-135806b52f83"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Levenshtein in /usr/local/lib/python3.11/dist-packages (0.27.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from Levenshtein) (3.12.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: einops_exts in /usr/local/lib/python3.11/dist-packages (0.0.4)\n",
            "Requirement already satisfied: einops>=0.4 in /usr/local/lib/python3.11/dist-packages (from einops_exts) (0.8.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from einops import rearrange, repeat\n",
        "# import esm\n",
        "\n",
        "# Set up GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# # Load ESM-2 model\n",
        "# esm_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D()\n",
        "# batch_converter = alphabet.get_batch_converter()\n",
        "# esm_model = esm_model.to(device)  # Move to GPU if available\n",
        "# esm_model.eval()  # Set to evaluation mode\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CYInyPD_--Vo",
        "outputId": "c5f4ca61-8d9d-4f2a-b0a7-7f1ac2586cb6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### data"
      ],
      "metadata": {
        "id": "B--CLs46QMLR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DyL8K36gQgPj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UGP9rWlQgn_",
        "outputId": "b2be12a2-8805-47ed-893c-163e134f1ffb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aN6Of5EMJ2Ju"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_snp_data(file_path):\n",
        "    snp_df = pd.read_csv(file_path)\n",
        "\n",
        "    # Basic preprocessing and length calculations\n",
        "    snp_df['smiles_length'] = snp_df['smiles'].apply(len)\n",
        "    snp_df['protein_length'] = snp_df['protein_sequence'].apply(len)\n",
        "\n",
        "    return snp_df\n",
        "\n",
        "def filter_datasets(dataset):\n",
        "    return dataset[\n",
        "        (dataset['smiles'].notna()) &\n",
        "        (dataset['protein_sequence'].notna()) &\n",
        "        (dataset['smiles_length'] > 0) &\n",
        "        (dataset['protein_length'] > 0)\n",
        "    ]\n",
        "\n",
        "class ProteinGenerationDataset(Dataset):\n",
        "    def __init__(self, dataframe, max_length):\n",
        "        self.dataframe = dataframe\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "        return row['smiles'], row['protein_sequence']\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function to handle padding within batches.\n",
        "    Args:\n",
        "        batch: List of tuples (smiles, protein)\n",
        "    Returns:\n",
        "        Padded and batched tensors\n",
        "    \"\"\"\n",
        "    smiles, proteins = zip(*batch)\n",
        "\n",
        "    # SMILES strings don't need padding as PolyBERT handles that internally\n",
        "    smiles = list(smiles)\n",
        "\n",
        "    # Get max length in this batch for proteins (not exceeding dataset max_length)\n",
        "    max_protein_len = min(max(len(p) for p in proteins), max_length)\n",
        "\n",
        "    # Pad proteins to max length in batch\n",
        "    padded_proteins = []\n",
        "    protein_masks = []\n",
        "\n",
        "    for protein in proteins:\n",
        "        if len(protein) > max_protein_len:\n",
        "            padded = protein[:max_protein_len]\n",
        "            mask = [1] * max_protein_len\n",
        "        else:\n",
        "            padded = protein + ' ' * (max_protein_len - len(protein))\n",
        "            mask = [1] * len(protein) + [0] * (max_protein_len - len(protein))\n",
        "\n",
        "        padded_proteins.append(padded)\n",
        "        protein_masks.append(mask)\n",
        "\n",
        "    return {\n",
        "        'smiles': smiles,\n",
        "        'proteins': padded_proteins,\n",
        "        'protein_masks': torch.tensor(protein_masks, dtype=torch.bool)\n",
        "    }"
      ],
      "metadata": {
        "id": "t6vR0DNfQRUZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### utils"
      ],
      "metadata": {
        "id": "Y84XLPeXQIEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Components\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0)]\n",
        "\n",
        "class DoublePositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        # Use the full embedding dimension divided into two halves\n",
        "        self.d_model = d_model\n",
        "        half_dim = d_model // 2\n",
        "\n",
        "        # Create position encodings for both input and output positions\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, half_dim, 2) * (-math.log(10000.0) / half_dim))\n",
        "\n",
        "        # Input position encodings\n",
        "        pe_input = torch.zeros(max_len, half_dim)\n",
        "        pe_input[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe_input[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Output position encodings\n",
        "        pe_output = torch.zeros(max_len, half_dim)\n",
        "        pe_output[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe_output[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        self.register_buffer('pe_input', pe_input)\n",
        "        self.register_buffer('pe_output', pe_output)\n",
        "\n",
        "    def forward(self, x, input_positions, output_positions):\n",
        "        batch_size, seq_length, _ = x.shape\n",
        "\n",
        "        # Create a tensor of zeros with the same shape as the input\n",
        "        pos_encoding = torch.zeros_like(x)\n",
        "\n",
        "        # For each item in the batch\n",
        "        for b in range(batch_size):\n",
        "            for t in range(seq_length):\n",
        "                # Get the input and output positions for this token\n",
        "                input_pos = input_positions[b, t] if input_positions is not None else t\n",
        "                output_pos = output_positions[b, t] if output_positions is not None else t\n",
        "\n",
        "                if input_pos < self.pe_input.size(0) and output_pos < self.pe_output.size(0):\n",
        "                    # Fill the first half with input position encoding\n",
        "                    pos_encoding[b, t, :self.d_model//2] = self.pe_input[input_pos]\n",
        "                    # Fill the second half with output position encoding\n",
        "                    pos_encoding[b, t, self.d_model//2:] = self.pe_output[output_pos]\n",
        "\n",
        "        return x + pos_encoding\n",
        "\n",
        "class PerceiverAttention(nn.Module):\n",
        "    def __init__(self, dim, dim_head=64, heads=8):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "        inner_dim = dim_head * heads\n",
        "\n",
        "        self.norm_media = nn.LayerNorm(dim)\n",
        "        self.norm_latents = nn.LayerNorm(dim)\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim, bias=False)\n",
        "\n",
        "    def forward(self, x, latents):\n",
        "        \"\"\"\n",
        "        x: [batch_size, seq_len_x, dim]\n",
        "        latents: [batch_size, seq_len_l, dim]\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        x = self.norm_media(x)\n",
        "        latents = self.norm_latents(latents)\n",
        "\n",
        "        # Ensure latents has correct batch size\n",
        "        if latents.size(0) != batch_size:\n",
        "            latents = latents.expand(batch_size, -1, -1)\n",
        "\n",
        "        q = self.to_q(latents)\n",
        "        q = rearrange(q, 'b n (h d) -> b h n d', h=self.heads)\n",
        "        q = q * self.scale\n",
        "\n",
        "        # Ensure proper concatenation\n",
        "        kv_input = torch.cat((x, latents), dim=1)  # concatenate along sequence dimension\n",
        "        k, v = self.to_kv(kv_input).chunk(2, dim=-1)\n",
        "        k = rearrange(k, 'b n (h d) -> b h n d', h=self.heads)\n",
        "        v = rearrange(v, 'b n (h d) -> b h n d', h=self.heads)\n",
        "\n",
        "        sim = torch.einsum('b h i d, b h j d -> b h i j', q, k)\n",
        "        attn = sim.softmax(dim=-1)\n",
        "        out = torch.einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "\n",
        "        return self.to_out(out)\n",
        "\n",
        "class GatedCrossAttentionBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_head=64, heads=8, ff_mult=4):\n",
        "        super().__init__()\n",
        "        self.attn = PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads)\n",
        "        self.attn_gate = nn.Parameter(torch.tensor([0.]))\n",
        "        self.ff = FeedForward(dim, mult=ff_mult)\n",
        "        self.ff_gate = nn.Parameter(torch.tensor([0.]))\n",
        "\n",
        "    def forward(self, x, media):\n",
        "        \"\"\"\n",
        "        x: [batch_size, seq_len_x, dim]\n",
        "        media: [batch_size, seq_len_m, dim]\n",
        "        \"\"\"\n",
        "        batch_size = x.shape[0]\n",
        "        target_batch_size = media.size(0)\n",
        "\n",
        "        # Handle batch size mismatch\n",
        "        if batch_size > target_batch_size:\n",
        "            media = media.expand(batch_size, -1, -1)\n",
        "        elif batch_size < target_batch_size:\n",
        "            x = x.expand(target_batch_size, -1, -1)\n",
        "\n",
        "        gate = self.attn_gate.tanh()\n",
        "        x = self.attn(media, x) * gate + x\n",
        "        x = self.ff(x) * self.ff_gate.tanh() + x\n",
        "        return x\n",
        "\n",
        "class PerceiverResampler(nn.Module):\n",
        "    def __init__(self, dim, depth, dim_head=64, heads=8, num_latents=64):\n",
        "        super().__init__()\n",
        "        # Initialize latents without batch dimension\n",
        "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n",
        "                FeedForward(dim=dim)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        # Expand latents to batch size\n",
        "        latents = repeat(self.latents, 'n d -> b n d', b=batch_size)\n",
        "\n",
        "        for attn, ff in self.layers:\n",
        "            latents = attn(x, latents) + latents\n",
        "            latents = ff(latents) + latents\n",
        "\n",
        "        return latents\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, mult=4):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, dim * mult, bias=False),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim * mult, dim, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# class PerceiverResampler(nn.Module):\n",
        "#     def __init__(self, dim, depth, dim_head=64, heads=8, num_latents=64):\n",
        "#         super().__init__()\n",
        "#         self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
        "#         self.layers = nn.ModuleList([])\n",
        "\n",
        "#         for _ in range(depth):\n",
        "#             self.layers.append(nn.ModuleList([\n",
        "#                 PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads),\n",
        "#                 FeedForward(dim=dim)\n",
        "#             ]))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         latents = repeat(self.latents, 'n d -> b n d', b=x.shape[0])\n",
        "\n",
        "#         for attn, ff in self.layers:\n",
        "#             latents = attn(x, latents) + latents\n",
        "#             latents = ff(latents) + latents\n",
        "\n",
        "#         return latents\n",
        "\n",
        "# class GatedCrossAttentionBlock(nn.Module):\n",
        "#     def __init__(self, dim, dim_head=64, heads=8, ff_mult=4):\n",
        "#         super().__init__()\n",
        "#         self.attn = PerceiverAttention(dim=dim, dim_head=dim_head, heads=heads)\n",
        "#         self.attn_gate = nn.Parameter(torch.tensor([0.]))\n",
        "#         self.ff = FeedForward(dim, mult=ff_mult)\n",
        "#         self.ff_gate = nn.Parameter(torch.tensor([0.]))\n",
        "\n",
        "#     def forward(self, x, media):\n",
        "#         gate = self.attn_gate.tanh()\n",
        "#         x = self.attn(media, x) * gate + x\n",
        "#         x = self.ff(x) * self.ff_gate.tanh() + x\n",
        "#         return x"
      ],
      "metadata": {
        "id": "XEODw7FYQJEy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PolyBert Encoder"
      ],
      "metadata": {
        "id": "daZm9aPyRUAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "00Y8KtCzni07"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class PolyBERTEncoder(nn.Module):\n",
        "#     def __init__(self, output_dim):\n",
        "#         super().__init__()\n",
        "#         self.polybert = AutoModel.from_pretrained('kuelumbus/polyBERT')\n",
        "#         self.tokenizer = AutoTokenizer.from_pretrained('kuelumbus/polyBERT')\n",
        "#         self.output_dim = output_dim\n",
        "#         # Add a projection layer to match the required dimension\n",
        "#         self.projection = nn.Linear(self.polybert.config.hidden_size, output_dim)\n",
        "\n",
        "#     def mean_pooling(self, model_output, attention_mask):\n",
        "#         token_embeddings = model_output[0]\n",
        "#         input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "#         return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "#     def forward(self, smiles_strings):\n",
        "#         # Tokenize the SMILES strings\n",
        "#         encoded_input = self.tokenizer(smiles_strings,\n",
        "#                                      padding=True,\n",
        "#                                      truncation=True,\n",
        "#                                      return_tensors='pt').to(next(self.polybert.parameters()).device)\n",
        "\n",
        "#         # Get PolyBERT embeddings\n",
        "#         with torch.no_grad():\n",
        "#             model_output = self.polybert(**encoded_input)\n",
        "\n",
        "#         # Debug prints\n",
        "#         print(\"Model Output Keys:\", model_output.keys())  # Check available keys\n",
        "#         # print(\"Last Hidden State:\", model_output.last_hidden_state)\n",
        "#         print(\"Last Hidden State Shape:\", model_output.last_hidden_state.shape)\n",
        "\n",
        "#         # Pool the embeddings\n",
        "#         pooled_output = self.mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "\n",
        "#         # print(\"Pooled Output:\", pooled_output)\n",
        "#         print(\"Pooled Output Shape:\", pooled_output.shape)\n",
        "\n",
        "#         # Project to required dimension\n",
        "#         projected_output = self.projection(pooled_output)\n",
        "\n",
        "#         return projected_output"
      ],
      "metadata": {
        "id": "rj3wpaKHRVmv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolyBERTEncoder(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super().__init__()\n",
        "        self.polybert = AutoModel.from_pretrained('kuelumbus/polyBERT')\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained('kuelumbus/polyBERT')\n",
        "        self.output_dim = output_dim\n",
        "        # Project each token embedding to required dimension\n",
        "        self.projection = nn.Linear(self.polybert.config.hidden_size, output_dim)\n",
        "\n",
        "    def forward(self, smiles_strings):\n",
        "        # Tokenize the SMILES strings\n",
        "        encoded_input = self.tokenizer(smiles_strings,\n",
        "                                     padding=True,\n",
        "                                     truncation=True,\n",
        "                                     return_tensors='pt').to(next(self.polybert.parameters()).device)\n",
        "\n",
        "        # Get PolyBERT embeddings\n",
        "        with torch.no_grad():\n",
        "            model_output = self.polybert(**encoded_input)\n",
        "\n",
        "        # Debug prints\n",
        "        # print(\"Model Output Keys:\", model_output.keys())\n",
        "        # print(\"Last Hidden State Shape:\", model_output.last_hidden_state.shape)  # [batch_size, seq_len, hidden_size]\n",
        "\n",
        "        # Get sequence embeddings\n",
        "        sequence_embeddings = model_output.last_hidden_state\n",
        "\n",
        "        # Project each token embedding to required dimension\n",
        "        projected_output = self.projection(sequence_embeddings)  # [batch_size, seq_len, output_dim]\n",
        "        # print(\"Projected Output Shape:\", projected_output.shape)\n",
        "\n",
        "        return projected_output"
      ],
      "metadata": {
        "id": "-2VWy9ecrDNX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ProtFlamingo"
      ],
      "metadata": {
        "id": "0Pe8VJFxQSS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n"
      ],
      "metadata": {
        "id": "d3FCpZZidXy3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "c1CcCRv-9vxW"
      },
      "outputs": [],
      "source": [
        "class SigmaProtFlamingo(nn.Module):\n",
        "    def __init__(self, model_path, max_len, cross_attn_every=3, dim_head=64, heads=8, perceiver_depth=2, perceiver_num_latents=64):\n",
        "        super().__init__()\n",
        "\n",
        "        self.protGPT2_model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "        self.protGPT2_tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "        self.max_len = max_len\n",
        "\n",
        "        if self.protGPT2_tokenizer.pad_token is None:\n",
        "            self.protGPT2_tokenizer.pad_token = self.protGPT2_tokenizer.eos_token\n",
        "            self.protGPT2_model.config.pad_token_id = self.protGPT2_model.config.eos_token_id\n",
        "\n",
        "        self.cross_attn_every = cross_attn_every\n",
        "\n",
        "        # PolyBERT encoder for SMILES strings\n",
        "        self.polybert_encoder = PolyBERTEncoder(self.protGPT2_model.config.n_embd)\n",
        "\n",
        "        # Replace single positional encoding with double positional encoding\n",
        "        self.positional_encoding = DoublePositionalEncoding(self.protGPT2_model.config.n_embd, max_len=max_len)\n",
        "\n",
        "        # Single perceiver resampler for SMILES embeddings\n",
        "        self.smiles_perceiver = PerceiverResampler(\n",
        "            dim=self.protGPT2_model.config.n_embd,\n",
        "            depth=perceiver_depth,\n",
        "            dim_head=dim_head,\n",
        "            heads=heads,\n",
        "            num_latents=perceiver_num_latents\n",
        "        )\n",
        "\n",
        "        # Cross attention layers\n",
        "        num_gpt_layers = len(self.protGPT2_model.transformer.h)\n",
        "        self.cross_attn = nn.ModuleList([\n",
        "            GatedCrossAttentionBlock(dim=self.protGPT2_model.config.n_embd, dim_head=dim_head, heads=heads)\n",
        "            for _ in range(num_gpt_layers)\n",
        "        ])\n",
        "\n",
        "        # Combine GPT layers with cross attention\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i, block in enumerate(self.protGPT2_model.transformer.h):\n",
        "            self.layers.append(block)\n",
        "            if i % cross_attn_every == 0 and i != 0:\n",
        "                self.layers.append(GatedCrossAttentionBlock(dim=self.protGPT2_model.config.n_embd, dim_head=dim_head, heads=heads))\n",
        "\n",
        "    def forward(self, smiles_strings, order=None, targets=None, optimize=False, kv_cache=None, burst=False):\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        # Get SMILES embeddings through PolyBERT\n",
        "        smiles_embeddings = self.polybert_encoder(smiles_strings)\n",
        "        processed_smiles = self.smiles_perceiver(smiles_embeddings)\n",
        "\n",
        "        # Initialize with start token\n",
        "        gpt_input = self.protGPT2_tokenizer.encode_plus(\n",
        "            \"<|endoftext|>\",\n",
        "            return_tensors=\"pt\",\n",
        "            padding='max_length',\n",
        "            max_length=self.max_len,\n",
        "            truncation=True\n",
        "        ).to(device)\n",
        "\n",
        "        input_ids = gpt_input.input_ids.long()\n",
        "        seq_length = input_ids.size(1)\n",
        "        batch_size = 1 if isinstance(smiles_strings, str) else len(smiles_strings)\n",
        "\n",
        "        hidden_states = self.protGPT2_model.transformer.wte(input_ids)\n",
        "\n",
        "        # If no order is provided, use left-to-right\n",
        "        if order is None:\n",
        "            order = torch.arange(seq_length, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "\n",
        "        # Make sure order is the right length\n",
        "        if order.size(1) > seq_length:\n",
        "            order = order[:, :seq_length]\n",
        "        elif order.size(1) < seq_length:\n",
        "            # Pad order if needed\n",
        "            padding = torch.arange(order.size(1), seq_length, device=device).unsqueeze(0).repeat(batch_size, 1)\n",
        "            order = torch.cat([order, padding], dim=1)\n",
        "\n",
        "        # Map the input tokens according to the order\n",
        "        # When using random order, we need to reshuffle the input tokens\n",
        "        if not optimize and not burst:  # Only shuffle during training\n",
        "            reordered_input_ids = torch.zeros_like(input_ids)\n",
        "            for b in range(batch_size):\n",
        "                # Reorder the input tokens according to the order\n",
        "                reordered_input_ids[b] = input_ids[b, order[b]]\n",
        "\n",
        "            # Re-embed with reordered tokens\n",
        "            hidden_states = self.protGPT2_model.transformer.wte(reordered_input_ids)\n",
        "\n",
        "        # Get input and output positions from the order\n",
        "        # Input positions: the current position in the order\n",
        "        # Output positions: the next position in the order\n",
        "        input_positions = order\n",
        "        # Shift the order by 1 to get output positions (target positions)\n",
        "        output_positions = torch.roll(order, -1, dims=1)\n",
        "        # The last position wraps to the first position\n",
        "        output_positions[:, -1] = order[:, 0]\n",
        "\n",
        "        # Apply double positional encoding\n",
        "        hidden_states = self.positional_encoding(hidden_states, input_positions, output_positions)\n",
        "\n",
        "        # Create attention mask based on the order\n",
        "        attention_mask = gpt_input.attention_mask\n",
        "        num_heads = self.protGPT2_model.config.n_head\n",
        "\n",
        "        # Create 4D attention mask [batch_size, num_heads, seq_length, seq_length]\n",
        "        attention_mask = attention_mask.view(batch_size, 1, 1, seq_length)\n",
        "        attention_mask = attention_mask.expand(batch_size, num_heads, seq_length, seq_length)\n",
        "        attention_mask = attention_mask.to(dtype=hidden_states.dtype)\n",
        "\n",
        "        # Create causal mask based on the order\n",
        "        # A token at position i can attend to tokens at positions j where order[j] <= order[i]\n",
        "        # Vectorized causal mask creation\n",
        "        seq_indices = torch.arange(seq_length, device=device)\n",
        "        expanded_seq_indices_i = seq_indices.unsqueeze(1).expand(seq_length, seq_length)\n",
        "        expanded_seq_indices_j = seq_indices.unsqueeze(0).expand(seq_length, seq_length)\n",
        "\n",
        "        causal_mask = torch.zeros((batch_size, seq_length, seq_length), device=device)\n",
        "        for b in range(batch_size):\n",
        "            # Get order for this batch\n",
        "            order_b = order[b]\n",
        "            # Get order values at positions i and j\n",
        "            order_i = order_b[expanded_seq_indices_i]\n",
        "            order_j = order_b[expanded_seq_indices_j]\n",
        "            # Create mask where order_j <= order_i\n",
        "            causal_mask[b] = (order_j <= order_i).float()\n",
        "\n",
        "        # Reshape causal_mask to match attention_mask and combine them\n",
        "        causal_mask = causal_mask.unsqueeze(1)  # [batch_size, 1, seq_length, seq_length]\n",
        "        combined_mask = attention_mask * causal_mask\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if isinstance(layer, GatedCrossAttentionBlock):\n",
        "                hidden_states = layer(hidden_states, processed_smiles)\n",
        "            else:\n",
        "                hidden_states = layer(hidden_states, attention_mask=combined_mask)[0]\n",
        "\n",
        "        # Get logits\n",
        "        logits = self.protGPT2_model.lm_head(hidden_states)\n",
        "\n",
        "        if targets is None:\n",
        "            if optimize:\n",
        "                # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "                return logits[:, [-1], :], None\n",
        "            return logits, None\n",
        "\n",
        "        # Compute loss against the targets\n",
        "        # If targets are provided in original order, we need to shuffle them to match our order\n",
        "        if targets is not None:\n",
        "            shuffled_targets = torch.zeros_like(targets)\n",
        "            for b in range(batch_size):\n",
        "                # Reorder the targets according to the order\n",
        "                shuffled_targets[b] = targets[b, order[b]]\n",
        "\n",
        "            loss = F.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                shuffled_targets.view(-1),\n",
        "                ignore_index=-1\n",
        "            )\n",
        "        else:\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def custom_generate(self, smiles_string, max_length=200):\n",
        "        device = next(self.parameters()).device\n",
        "\n",
        "        # Get SMILES embeddings\n",
        "        smiles_embeddings = self.polybert_encoder(smiles_string)\n",
        "        processed_smiles = self.smiles_perceiver(smiles_embeddings)\n",
        "\n",
        "        # Initialize with start token\n",
        "        input_ids = torch.tensor([[self.protGPT2_tokenizer.bos_token_id]]).to(device)\n",
        "\n",
        "        # Autoregressive generation\n",
        "        for _ in range(max_length):\n",
        "            inputs_embeds = self.protGPT2_model.transformer.wte(input_ids)\n",
        "            inputs_embeds = self.positional_encoding(inputs_embeds)\n",
        "\n",
        "            hidden_states = inputs_embeds\n",
        "            cross_attn_idx = 0\n",
        "\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                if isinstance(layer, GatedCrossAttentionBlock):\n",
        "                    hidden_states = layer(hidden_states, processed_smiles)\n",
        "                    cross_attn_idx += 1\n",
        "                else:\n",
        "                    hidden_states = layer(hidden_states, attention_mask=None)[0]\n",
        "\n",
        "            next_token_logits = self.protGPT2_model.lm_head(hidden_states[:, -1, :])\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0)\n",
        "\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
        "\n",
        "            if next_token.item() == self.protGPT2_tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        return self.protGPT2_tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    def generate(self, smiles_string, max_length=50):\n",
        "        return self.custom_generate(smiles_string, max_length)\n",
        "\n",
        "    def state_dict(self):\n",
        "        state_dict = super().state_dict()\n",
        "        state_dict['smiles_perceiver'] = self.smiles_perceiver.state_dict()\n",
        "        state_dict['cross_attn'] = self.cross_attn.state_dict()\n",
        "        state_dict['polybert_encoder'] = self.polybert_encoder.state_dict()\n",
        "        return state_dict\n",
        "\n",
        "    def load_state_dict(self, state_dict):\n",
        "        smiles_perceiver_state = state_dict.pop('smiles_perceiver')\n",
        "        cross_attn_state = state_dict.pop('cross_attn')\n",
        "        polybert_encoder_state = state_dict.pop('polybert_encoder')\n",
        "\n",
        "        super().load_state_dict(state_dict)\n",
        "\n",
        "        self.smiles_perceiver.load_state_dict(smiles_perceiver_state)\n",
        "        self.cross_attn.load_state_dict(cross_attn_state)\n",
        "        self.polybert_encoder.load_state_dict(polybert_encoder_state)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random"
      ],
      "metadata": {
        "id": "_wnL43mTZkIN"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### training"
      ],
      "metadata": {
        "id": "6o91I0EXQaF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "wWmS0warQa-n"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_with_random_order(model, train_loader, val_loader, num_epochs, device,\n",
        "                           curriculum_steps=0, l2_reg=1e-5, sample_smiles=None):\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=l2_reg)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=model.protGPT2_tokenizer.pad_token_id)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    loss_log = []\n",
        "    checkpoint_path = \"/content/drive/MyDrive/classes+projects/plastic_enzyme_project/2024/codes/sigma_checkpoint.pth\"\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        print(\"Loading checkpoint...\")\n",
        "        model.load_state_dict(torch.load(checkpoint_path))\n",
        "\n",
        "    # Total training steps for curriculum\n",
        "    total_steps = num_epochs * len(train_loader)\n",
        "    step_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        batch_losses = []\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "            step_counter += 1\n",
        "\n",
        "            smiles_strings = batch['smiles']\n",
        "            proteins = batch['proteins']\n",
        "            protein_masks = batch['protein_masks'].to(device)\n",
        "\n",
        "            # Determine whether to use random or left-to-right order based on curriculum\n",
        "            if curriculum_steps > 0:\n",
        "                # Current percentage of random ordering\n",
        "                random_prob = min(1.0, step_counter / curriculum_steps)\n",
        "                use_random = random.random() < random_prob\n",
        "            else:\n",
        "                use_random = True\n",
        "\n",
        "            batch_size = len(smiles_strings)\n",
        "            seq_length = model.max_len\n",
        "\n",
        "            # Generate order\n",
        "            if use_random:\n",
        "                # Create random permutation for each batch item\n",
        "                order = torch.stack([torch.randperm(seq_length) for _ in range(batch_size)]).to(device)\n",
        "            else:\n",
        "                # Left-to-right order\n",
        "                order = torch.arange(seq_length).unsqueeze(0).repeat(batch_size, 1).to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Encode the proteins\n",
        "            target_encoding = model.protGPT2_tokenizer(\n",
        "                proteins,\n",
        "                return_tensors=\"pt\",\n",
        "                padding='max_length',\n",
        "                max_length=model.max_len,\n",
        "                truncation=True\n",
        "            ).to(device)\n",
        "\n",
        "            # Forward pass with order - the targets are already in original order\n",
        "            # The model will handle shuffling the targets according to the order\n",
        "            outputs, loss = model(\n",
        "                smiles_strings,\n",
        "                order=order,\n",
        "                targets=target_encoding.input_ids\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            batch_losses.append(loss.item())\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "        print(f\"Per-batch Losses: {batch_losses[:5]} ...\")\n",
        "\n",
        "        val_loss = validate_with_random_order(model, val_loader, criterion, device)\n",
        "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "        loss_log.append({\n",
        "            'epoch': epoch+1,\n",
        "            'train_loss': avg_loss,\n",
        "            'val_loss': val_loss\n",
        "        })\n",
        "\n",
        "        scheduler.step()\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "\n",
        "        # Generate sample proteins after each epoch\n",
        "        if sample_smiles:\n",
        "            print(\"\\nSample Generated Proteins:\")\n",
        "            for smiles in sample_smiles:\n",
        "                # Try both random and left-to-right generation\n",
        "                gen_protein_lr = generate_autoregressively(model, smiles, max_length=100, random_order=False)\n",
        "                gen_protein_random = generate_autoregressively(model, smiles, max_length=100, random_order=True)\n",
        "                print(f\"SMILES: {smiles}\")\n",
        "                print(f\"Generated Protein (L2R): {gen_protein_lr}\")\n",
        "                print(f\"Generated Protein (Random): {gen_protein_random}\\n\")\n",
        "\n",
        "    # Save loss log\n",
        "    loss_df = pd.DataFrame(loss_log)\n",
        "    loss_df.to_csv(\"/content/drive/MyDrive/classes+projects/plastic_enzyme_project/2024/codes/sigma_loss_log.csv\", index=False)\n",
        "\n",
        "    # Plot loss\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(loss_df['epoch'], loss_df['train_loss'], label='Train Loss', marker='o')\n",
        "    plt.plot(loss_df['epoch'], loss_df['val_loss'], label='Validation Loss', marker='s')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Training and Validation Loss\")\n",
        "    plt.savefig(\"sigma_loss_plot.png\")\n",
        "    plt.show()\n",
        "\n",
        "def validate_with_random_order(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            smiles_strings = batch['smiles']\n",
        "            proteins = batch['proteins']\n",
        "            protein_masks = batch['protein_masks'].to(device)\n",
        "\n",
        "            batch_size = len(smiles_strings)\n",
        "            seq_length = model.max_len\n",
        "\n",
        "            # Create random order for each item in the batch\n",
        "            order = torch.stack([torch.randperm(seq_length) for _ in range(batch_size)]).to(device)\n",
        "\n",
        "            # Encode the proteins\n",
        "            target_encoding = model.protGPT2_tokenizer(\n",
        "                proteins,\n",
        "                return_tensors=\"pt\",\n",
        "                padding='max_length',\n",
        "                max_length=model.max_len,\n",
        "                truncation=True\n",
        "            ).to(device)\n",
        "\n",
        "            # Forward pass with order\n",
        "            outputs, loss = model(\n",
        "                smiles_strings,\n",
        "                order=order,\n",
        "                targets=target_encoding.input_ids\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(val_loader)"
      ],
      "metadata": {
        "id": "M6WRIYHEfN6y"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### inference + training"
      ],
      "metadata": {
        "id": "CfYBZUMhfOTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = preprocess_snp_data('/content/augmented_train.csv')\n",
        "val_data = preprocess_snp_data('/content/augmented_val.csv')\n",
        "test_data = preprocess_snp_data('/content/augmented_test.csv')\n",
        "\n",
        "train_data = filter_datasets(train_data)\n",
        "val_data = filter_datasets(val_data)\n",
        "test_data = filter_datasets(test_data)\n",
        "\n",
        "# Calculate max sequence length\n",
        "max_length = max(\n",
        "    train_data['protein_length'].max(),\n",
        "    val_data['protein_length'].max(),\n",
        "    test_data['protein_length'].max()\n",
        ")\n",
        "max_length = min(max_length, 1024)  # Cap at 1024 or your desired maximum\n",
        "print(f\"Max sequence length: {max_length}\")\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ProteinGenerationDataset(train_data, max_length)\n",
        "val_dataset = ProteinGenerationDataset(val_data, max_length)\n",
        "test_dataset = ProteinGenerationDataset(test_data, max_length)\n",
        "\n",
        "# Create dataloaders with custom collate function\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,  # Adjust based on your GPU memory\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Initialize model with sigma-gpt capabilities\n",
        "model = SigmaProtFlamingo(\n",
        "    model_path='nferruz/ProtGPT2',\n",
        "    max_len=max_length,\n",
        "    cross_attn_every=3,\n",
        "    dim_head=64,\n",
        "    heads=8,\n",
        "    perceiver_depth=2,\n",
        "    perceiver_num_latents=64\n",
        ").to(device)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "# Training loop with curriculum learning\n",
        "# Start with 50% of sequences in left-to-right order and gradually increase to 100% random\n",
        "curriculum_steps = int(0.5 * num_epochs * len(train_loader))  # Curriculum over first half of training\n",
        "print(\"Starting training with sigma-gpt capabilities...\")\n",
        "train_with_random_order(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    num_epochs,\n",
        "    device,\n",
        "    curriculum_steps=curriculum_steps\n",
        ")\n",
        "\n",
        "# # Generate and evaluate\n",
        "# print(\"Generating proteins for test set...\")\n",
        "# test_results = generate_and_evaluate(model, test_loader, device)\n",
        "\n",
        "# # Save results\n",
        "# print(\"Saving results...\")\n",
        "# results_path = '/content/drive/MyDrive/classes+projects/plastic_enzyme_project/2024/codes/test_results.json'\n",
        "# with open(results_path, 'w') as f:\n",
        "#     json.dump(test_results, f, indent=2)\n",
        "\n",
        "# print(f\"Results saved to {results_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "e-67RvdafPdi",
        "outputId": "cd7dd272-bb5c-4aa0-a474-bf9dbf30d7bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Max sequence length: 914\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-4b64b00596ea>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Initialize model with sigma-gpt capabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m model = SigmaProtFlamingo(\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nferruz/ProtGPT2'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-e0bcda143273>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, max_len, cross_attn_every, dim_head, heads, perceiver_depth, perceiver_num_latents)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotGPT2_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotGPT2_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3991\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sharded\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3992\u001b[0m                 \u001b[0;31m# Time to load the checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3993\u001b[0;31m                 \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3995\u001b[0m             \u001b[0;31m# set dtype to instantiate the model under:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[1;32m    533\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"mmap\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0mweights_only_kwarg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"weights_only\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         return torch.load(\n\u001b[0m\u001b[1;32m    536\u001b[0m             \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m             \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m                         return _load(\n\u001b[0m\u001b[1;32m   1352\u001b[0m                             \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m                             \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_weights_only_unpickler.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\">d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mBINUNICODE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0mstrlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mstrlen\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"String is too long\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hsqe5zYSJ5Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### generation"
      ],
      "metadata": {
        "id": "sLZXFhNq9xvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the trained model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Path to model checkpoint\n",
        "checkpoint_path = \"/content/drive/MyDrive/classes+projects/plastic_enzyme_project/2024/codes/sigma_checkpoint.pth\"\n",
        "\n",
        "# Load model\n",
        "model = SigmaProtFlamingo(\n",
        "    model_path='nferruz/ProtGPT2',\n",
        "    max_len=914,  # Ensure this matches the training max_len\n",
        "    cross_attn_every=3,\n",
        "    dim_head=64,\n",
        "    heads=8,\n",
        "    perceiver_depth=2,\n",
        "    perceiver_num_latents=64\n",
        ").to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "vXlyfekIsTlB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ProteinGenerationDataset"
      ],
      "metadata": {
        "id": "OS0GAkWg-aV3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "e5f22439-0187-45f1-88ba-6c8069beaaac"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.ProteinGenerationDataset"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>ProteinGenerationDataset</b><br/>def __init__(dataframe, max_length)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\"></a>An abstract class representing a :class:`Dataset`.\n",
              "\n",
              "All datasets that represent a map from keys to data samples should subclass\n",
              "it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
              "data sample for a given key. Subclasses could also optionally overwrite\n",
              ":meth:`__len__`, which is expected to return the size of the dataset by many\n",
              ":class:`~torch.utils.data.Sampler` implementations and the default options\n",
              "of :class:`~torch.utils.data.DataLoader`. Subclasses could also\n",
              "optionally implement :meth:`__getitems__`, for speedup batched samples\n",
              "loading. This method accepts list of indices of samples of batch and returns\n",
              "list of samples.\n",
              "\n",
              ".. note::\n",
              "  :class:`~torch.utils.data.DataLoader` by default constructs an index\n",
              "  sampler that yields integral indices.  To make it work with a map-style\n",
              "  dataset with non-integral indices/keys, a custom sampler must be provided.</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained weights\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Load test data\n",
        "test_data = preprocess_snp_data('/content/augmented_test.csv')\n",
        "test_data = filter_datasets(test_data)\n",
        "\n",
        "# Create test dataset and dataloader\n",
        "test_dataset = ProteinGenerationDataset(test_data,max_length = 914 )\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "JHBmfbMT-FPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed3c54ca-c324-48ac-d89d-7732a30b7fe4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-652a61e78837>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_autoregressively(model, smiles_string, max_length=914, temperature=1.0, random_order=False):\n",
        "    \"\"\"Generate protein autoregressively, with option to use random order\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Get SMILES embeddings\n",
        "    smiles_embeddings = model.polybert_encoder([smiles_string])\n",
        "    processed_smiles = model.smiles_perceiver(smiles_embeddings)\n",
        "\n",
        "    # Initialize with start token\n",
        "    input_ids = torch.tensor([[model.protGPT2_tokenizer.bos_token_id]], device=device)\n",
        "\n",
        "    # If using random order, generate a random permutation\n",
        "    if random_order:\n",
        "        order = torch.randperm(max_length, device=device).unsqueeze(0)\n",
        "    else:\n",
        "        order = torch.arange(max_length, device=device).unsqueeze(0)\n",
        "\n",
        "    # Track the current positions in the order\n",
        "    current_pos = 0\n",
        "\n",
        "    # Generated sequence in order's positions\n",
        "    generated_sequence = torch.full((1, max_length), model.protGPT2_tokenizer.pad_token_id, device=device)\n",
        "    generated_sequence[0, 0] = model.protGPT2_tokenizer.bos_token_id  # Start token\n",
        "\n",
        "    while current_pos < max_length - 1:\n",
        "        # Get the next position in the order\n",
        "        next_pos = current_pos + 1\n",
        "\n",
        "        # Forward pass to get next token prediction\n",
        "        with torch.no_grad():\n",
        "            # Use only the sequence up to the current position\n",
        "            current_order = order[:, :next_pos]\n",
        "            current_sequence = generated_sequence[:, current_order[0]]\n",
        "\n",
        "            # Get logits for the next token\n",
        "            logits, _ = model(\n",
        "                smiles_string,\n",
        "                order=current_order,\n",
        "                optimize=True\n",
        "            )\n",
        "\n",
        "            # Apply temperature and sample\n",
        "            logits = logits[0, -1, :] / temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "            # Add the token to the generated sequence at the next position in the order\n",
        "            generated_sequence[0, order[0, next_pos]] = next_token\n",
        "\n",
        "            # Check for EOS token\n",
        "            if next_token == model.protGPT2_tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            current_pos = next_pos\n",
        "\n",
        "    # Decode the generated sequence\n",
        "    generated_ids = generated_sequence[0].tolist()\n",
        "    # Remove padding tokens\n",
        "    generated_ids = [id for id in generated_ids if id != model.protGPT2_tokenizer.pad_token_id]\n",
        "    return model.protGPT2_tokenizer.decode(generated_ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "Sgmyq7fJ8kAb"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_with_rejection_sampling(model, smiles_string, max_length=914, num_orders=5, temperature=1.0):\n",
        "    \"\"\"Generate protein using token-based rejection sampling with proper MH acceptance ratio\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Get SMILES embeddings\n",
        "    smiles_embeddings = model.polybert_encoder([smiles_string])\n",
        "    processed_smiles = model.smiles_perceiver(smiles_embeddings)\n",
        "\n",
        "    # Initialize with start token\n",
        "    prompt = torch.tensor([[model.protGPT2_tokenizer.bos_token_id]], device=device)\n",
        "\n",
        "    # Initialize full sequence with padding\n",
        "    full_seq = torch.full((1, max_length), model.protGPT2_tokenizer.pad_token_id, device=device)\n",
        "    full_seq[:, 0] = model.protGPT2_tokenizer.bos_token_id  # Start token\n",
        "\n",
        "    # Track positions that have been filled\n",
        "    filled_positions = {0}  # Start with position 0 filled\n",
        "\n",
        "    while len(filled_positions) < max_length:\n",
        "        remaining_positions = [i for i in range(max_length) if i not in filled_positions]\n",
        "        if not remaining_positions:\n",
        "            break\n",
        "\n",
        "        # Step 1: Sample tokens at all remaining positions from marginal distribution\n",
        "        # This is our proposal distribution p(x)\n",
        "        candidate_tokens = {}\n",
        "        proposal_probs = {}  # Store the probability of each proposal\n",
        "\n",
        "        for pos in remaining_positions:\n",
        "            # Create current filled sequence context\n",
        "            current_context = torch.ones((1, max_length), device=device) * model.protGPT2_tokenizer.pad_token_id\n",
        "            for filled_pos in filled_positions:\n",
        "                current_context[0, filled_pos] = full_seq[0, filled_pos]\n",
        "\n",
        "            # Get logits for this position given current context\n",
        "            with torch.no_grad():\n",
        "                # Order that puts this position last\n",
        "                context_order = torch.tensor([list(filled_positions) + [pos]], device=device)\n",
        "\n",
        "                logits = get_logits_for_position(model, current_context, context_order, smiles_string, pos)\n",
        "\n",
        "                # Sample a token and record its probability\n",
        "                logits = logits / temperature\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                token_dist = torch.distributions.Categorical(probs)\n",
        "                token = token_dist.sample().item()\n",
        "\n",
        "                candidate_tokens[pos] = token\n",
        "                proposal_probs[pos] = probs[0, token].item()\n",
        "\n",
        "        # Step 2: Evaluate acceptance under different orders\n",
        "        best_order_acceptances = []\n",
        "\n",
        "        for _ in range(num_orders):\n",
        "            # Create a random permutation of remaining positions\n",
        "            eval_order = random.sample(remaining_positions, len(remaining_positions))\n",
        "\n",
        "            accepted_tokens = []\n",
        "            accepted_positions = []\n",
        "            acceptance_ratios = []\n",
        "\n",
        "            # Try to accept tokens in this order\n",
        "            for pos in eval_order:\n",
        "                # Create sequence with previously accepted tokens\n",
        "                temp_seq = full_seq.clone()\n",
        "                for acc_pos in accepted_positions:\n",
        "                    temp_seq[0, acc_pos] = candidate_tokens[acc_pos]\n",
        "\n",
        "                # Get conditional probability q(x|X,x<i)\n",
        "                filled_plus_accepted = list(filled_positions) + accepted_positions\n",
        "                context_order = torch.tensor([filled_plus_accepted + [pos]], device=device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    cond_logits = get_logits_for_position(\n",
        "                        model, temp_seq, context_order, processed_smiles, pos\n",
        "                    )\n",
        "\n",
        "                    cond_probs = F.softmax(cond_logits / temperature, dim=-1)\n",
        "                    cond_prob = cond_probs[0, candidate_tokens[pos]].item()\n",
        "\n",
        "                # Compute acceptance ratio r = q(xi|X,x<i) / p(xi|X)\n",
        "                # Where p(xi|X) is the proposal probability\n",
        "                acceptance_ratio = min(1.0, cond_prob / proposal_probs[pos])\n",
        "\n",
        "                # Decide whether to accept\n",
        "                if random.random() < acceptance_ratio:\n",
        "                    accepted_tokens.append(candidate_tokens[pos])\n",
        "                    accepted_positions.append(pos)\n",
        "                    acceptance_ratios.append(acceptance_ratio)\n",
        "                else:\n",
        "                    # Stop at first rejection\n",
        "                    break\n",
        "\n",
        "            best_order_acceptances.append((accepted_positions, accepted_tokens, acceptance_ratios))\n",
        "\n",
        "        # Step 3: Dynamic token acceptance\n",
        "        best_order_idx = -1\n",
        "        max_accepted = -1\n",
        "        min_sequence_idx = -1\n",
        "\n",
        "        for idx, (accepted_positions, _, acceptance_ratios) in enumerate(best_order_acceptances):\n",
        "            if len(accepted_positions) > max_accepted:\n",
        "                max_accepted = len(accepted_positions)\n",
        "                best_order_idx = idx\n",
        "                # Find the minimum position in the sequence where we see a rejection\n",
        "                if len(accepted_positions) < len(remaining_positions):\n",
        "                    min_sequence_idx = len(accepted_positions)\n",
        "                else:\n",
        "                    min_sequence_idx = len(remaining_positions)\n",
        "\n",
        "        # No need to calculate min across orders if all orders accept all tokens\n",
        "        if min_sequence_idx == -1:\n",
        "            min_sequence_idx = len(remaining_positions)\n",
        "\n",
        "        # Get the best order\n",
        "        best_order = best_order_acceptances[best_order_idx]\n",
        "        accepted_positions, accepted_tokens, _ = best_order\n",
        "\n",
        "        # Limit acceptance to positions before the minimum rejection\n",
        "        accepted_positions = accepted_positions[:min_sequence_idx]\n",
        "        accepted_tokens = accepted_tokens[:min_sequence_idx]\n",
        "\n",
        "        # Update the sequence with accepted tokens\n",
        "        for pos, token in zip(accepted_positions, accepted_tokens):\n",
        "            full_seq[0, pos] = token\n",
        "            filled_positions.add(pos)\n",
        "\n",
        "            # Check for EOS token\n",
        "            if token == model.protGPT2_tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # Decode the generated sequence\n",
        "    result = model.protGPT2_tokenizer.decode(\n",
        "        [t for t in full_seq[0].tolist() if t != model.protGPT2_tokenizer.pad_token_id],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    return result\n",
        "\n",
        "def get_logits_for_position(model, sequence, order, smiles_string, target_position):\n",
        "    \"\"\"Helper function to get logits for a specific position\"\"\"\n",
        "    # Run model forward pass\n",
        "    logits, _ = model(\n",
        "        smiles_string,  # Pass the SMILES string\n",
        "        order=order,\n",
        "        optimize=True\n",
        "    )\n",
        "\n",
        "    # Return logits for target position (last position in the order)\n",
        "    return logits[:, -1, :]"
      ],
      "metadata": {
        "id": "Koy4bjRHu5hx"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "IhpdI0vMXdUp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_on_unique_smiles(model, test_loader, device, output_file=\"generated_proteins_comparison.json\"):\n",
        "    \"\"\"Generate proteins using both methods on unique SMILES from test set\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Collect unique SMILES from the test loader\n",
        "    unique_smiles = set()\n",
        "    for batch in test_loader:\n",
        "        unique_smiles.update(batch['smiles'])\n",
        "\n",
        "    unique_smiles = list(unique_smiles)  # Convert to list\n",
        "    print(f\"Found {len(unique_smiles)} unique SMILES in test set\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Generate proteins using both methods and time each generation\n",
        "    for i, smiles in enumerate(tqdm(unique_smiles, desc=\"Generating proteins\")):\n",
        "        # Track time for autoregressive generation\n",
        "        start_time = time.time()\n",
        "        ar_protein = generate_autoregressively(model, smiles, max_length=914, temperature=1.0, random_order=False)\n",
        "        ar_time = time.time() - start_time\n",
        "\n",
        "        # Track time for rejection sampling\n",
        "        start_time = time.time()\n",
        "        rs_protein = generate_with_rejection_sampling(model, smiles, max_length=914, num_orders=5, temperature=1.0)\n",
        "        rs_time = time.time() - start_time\n",
        "\n",
        "        results.append({\n",
        "            'SMILES': smiles,\n",
        "            'Autoregressive': {\n",
        "                'protein': ar_protein,\n",
        "                'time_seconds': ar_time\n",
        "            },\n",
        "            'Rejection_Sampling': {\n",
        "                'protein': rs_protein,\n",
        "                'time_seconds': rs_time\n",
        "            }\n",
        "        })\n",
        "\n",
        "        # Print progress occasionally\n",
        "        if (i + 1) % 5 == 0:\n",
        "            print(f\"\\nCompleted {i+1}/{len(unique_smiles)}\")\n",
        "            print(f\"Example - SMILES: {smiles}\")\n",
        "            print(f\"Autoregressive: {ar_protein[:50]}... ({ar_time:.2f}s)\")\n",
        "            print(f\"Rejection Sampling: {rs_protein[:50]}... ({rs_time:.2f}s)\")\n",
        "\n",
        "    # Save results to JSON\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    # Calculate and print average times\n",
        "    ar_times = [r['Autoregressive']['time_seconds'] for r in results]\n",
        "    rs_times = [r['Rejection_Sampling']['time_seconds'] for r in results]\n",
        "\n",
        "    print(f\"\\nGeneration complete!\")\n",
        "    print(f\"Average autoregressive generation time: {np.mean(ar_times):.2f}s\")\n",
        "    print(f\"Average rejection sampling generation time: {np.mean(rs_times):.2f}s\")\n",
        "    print(f\"Speed improvement: {np.mean(ar_times)/np.mean(rs_times):.2f}x\")\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "2azHLfyMXUyR"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = evaluate_on_unique_smiles(model, test_loader, device, output_file=\"sigma_gpt_comparison_results.json\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xKIwu3LXe7K",
        "outputId": "4e43d502-7106-4cc3-ac80-a9b4a06890a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 unique SMILES in test set\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rGenerating proteins:   0%|          | 0/9 [00:00<?, ?it/s]"
          ]
        }
      ]
    }
  ]
}